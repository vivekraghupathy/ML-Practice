{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n",
      "is\n",
      "my\n",
      "country\n",
      ".\n",
      "is\n",
      "is my\n"
     ]
    }
   ],
   "source": [
    "#Create a document object & tokenize\n",
    "doc = nlp('India is my country.')\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "#Access token with index on doc object\n",
    "print(doc[1])\n",
    "#Slice the doc like a python list\n",
    "print(doc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Attributes\n",
    "- is_alpha determines if the token is alphabets\n",
    "- is_punct determines if it is a punctuation\n",
    "- like_num tells if the token is numeric\n",
    "- is_currency tells if it is a currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5]\n",
      "Text:     ['A', 'pizza', 'costs', '$', '10.5', '.']\n",
      "is_alpha: [True, True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, False, True]\n",
      "like_num: [False, False, False, False, True, False]\n",
      "like_num: [False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A pizza costs $10.5.\")\n",
    "#Print index\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "#Print text tokens\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "#Check if token is alpha\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "#Check if token is punctuation\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "#Check of token is like a number\n",
    "print('like_num:', [token.like_num for token in doc])\n",
    "#Check if token is a currency\n",
    "print('is_currency:', [token.is_currency for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Model for Language\n",
    "- token.pos_: will give the part of speech tags\n",
    "- token.dep_:  Dependency relation connecting the root to its head\n",
    "- token.head.text: Gives the main word to which the token is related\n",
    "- doc.ents: has the identified named entities\n",
    "- explain function can be used to get the meaning of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET\n",
      "pizza NOUN\n",
      "costs VERB\n",
      "$ SYM\n",
      "10.5 NUM\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "#Part of Speech\n",
    "doc = nlp(\"A pizza costs $10.5.\")\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj looking\n",
      "is VERB aux looking\n",
      "looking VERB ROOT looking\n",
      "at ADP prep looking\n",
      "buying VERB pcomp at\n",
      "U.K. PROPN compound startup\n",
      "startup NOUN dobj buying\n",
      "for ADP prep buying\n",
      "$ SYM quantmod billion\n",
      "1 NUM compound billion\n",
      "billion NUM pobj for\n"
     ]
    }
   ],
   "source": [
    "#Syntactic Dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#Named Entities\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monetary values, including unit'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explanation about entities & pos tags \n",
    "spacy.explain('MONEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based matching\n",
    "- Used for searching text, their lexical attributes, models prediction of the word\n",
    "- Match Patterns: list of dictionaries. Each dictionary defines a token attribute and its value to match\n",
    "- Matcher will return match_id, start index, end index of match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "#initialize the matcher with a vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "#Create a pattern dictionary list. The below pattern searches for Tokens 'TEXT' attribute with value 'iPhone' & 'X'\n",
    "pattern = [{'TEXT':'iPhone'},{'TEXT':'X'}]\n",
    "#Add this pattern to our matcher. add function has #Name of Pattern, #Callback #actual pattern\n",
    "matcher.add('PATTERN_IPHONE',None,pattern)\n",
    "#Use the pattern for matching\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "#Iterate over the matcher\n",
    "for match_id, start, end in matches:\n",
    "    matched = doc[start:end]\n",
    "    print(matched.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "#Pattern with digit and punctuation\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "matcher.add('PATTERN_IPHONE',None,pattern)\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched = doc[start:end]\n",
    "    print(matched.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n",
      "love pizza\n"
     ]
    }
   ],
   "source": [
    "#Pattern using LEMMA, POS\n",
    "pattern = [\n",
    "    {'LEMMA': 'love'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add('PATTERN_IPHONE',None,pattern)\n",
    "doc = nlp(\"I loved dogs but now I love cats more. I love pizza. I love to travel.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched = doc[start:end]\n",
    "    print(matched.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful design\n",
      "smart search\n",
      "automatic labels\n",
      "optional voice responses\n"
     ]
    }
   ],
   "source": [
    "#Pattern ADJ followed by a NOUN and an optional next NOUN\n",
    "pattern = [\n",
    "    {'POS':'ADJ'},\n",
    "    {'POS':'NOUN'},\n",
    "    {'POS':'NOUN','OP':'?'}\n",
    "]\n",
    "matcher.add('PATTERN_IPHONE',None,pattern)\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched = doc[start:end]\n",
    "    print(matched.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "- Compares 2 items and gives the similarity score(0 to 1). The items can be doc, slice or a token\n",
    "- This uses word vectors which are included in core medium or large models (models ending with 'md' or 'lg')\n",
    "- Spacy uses cosine similarity between two vectors give the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "#Compare doc to doc\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6446904\n"
     ]
    }
   ],
   "source": [
    "#Compare token to token\n",
    "doc = nlp(\"I like France & England\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5286024505739466\n"
     ]
    }
   ],
   "source": [
    "#Compare doc to token\n",
    "doc = nlp('I like pizza.')\n",
    "token =nlp('pasta')\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "- Multidimensional representation of meanings of word\n",
    "- Word2Vec is Neural network model to build word Vectors. It predicts the context given a word. This will help us get similar words in same context.\n",
    "- If the model is trained using 300 neuron hidden layer then we get a 300 element vector for each word.\n",
    "- the doc.vector attribute gives us the vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "#The 300 vector representation of word banana\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632563384120956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem of using default similarity measure.\n",
    "doc1 = nlp('I like cats.')\n",
    "doc2 = nlp('I hate cats.')\n",
    "doc1.similarity(doc2)\n",
    "#The score says they are very similar because both talks about feeling towards cat. But the sentiment is negative here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spacy Pipeline\n",
    "- when we instantiate en_core_web_md and pass a text it executes a pipeline and create a doc\n",
    "- the pipeline consists of:\n",
    "    - tokenizer\n",
    "    - tagger\n",
    "    - parser\n",
    "    - ner\n",
    "- we can add custom components to this pipeline. Define a function to do some action on doc object. Then add it to the pipeline using add_pipe \n",
    "- position of the custom component is determined by keyword last, first, after, before of add_pipe method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myComponent', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "#Define component action using function\n",
    "def myComponent(doc):\n",
    "    print('Doc Length:', len(doc))\n",
    "    return doc\n",
    "\n",
    "#Add component to pipeline\n",
    "nlp.add_pipe(myComponent,first=True)\n",
    "\n",
    "#Print pipeline components\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Length: 3\n"
     ]
    }
   ],
   "source": [
    "#create the doc item and see our component in action\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model for own data\n",
    "- This is needed if the model need to perform better on our domain\n",
    "- The pretrained models are very generic\n",
    "- This is more critical for Named Entity Recognition\n",
    "### Steps in training\n",
    "    - Initialize the model weights randomly with nlp.begin_training\n",
    "    - Predict a few examples with the current weights by calling nlp.update\n",
    "    - Compare prediction with true labels\n",
    "    - Calculate how to change weights to improve predictions\n",
    "    - Update weights slightly\n",
    "    - Go back to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I need a new phone! Any tips?', {'entities': []})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating training data for NER. Spacy needs data as a tuple\n",
    "#We need the text and the entity start position, end position and the name of entity\n",
    "(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]})\n",
    "#Also we need some cases where there are no entities to extract\n",
    "(\"I need a new phone! Any tips?\", {'entities': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The above training data can be automatically created using pattern mathcer\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "#Create training data using matchers created above\n",
    "TRAINING_DATA = []\n",
    "TEXTS = ['How to preorder the iPhone X', \n",
    "         'iPhone X is coming', \n",
    "         'Should I pay $1,000 for the iPhone X?', \n",
    "         'The iPhone 8 reviews are here', \n",
    "         'Your iPhone goes up to 11 today', \n",
    "         'I need a new phone! Any tips?']\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Loop for 10 iterations\n",
    "for itn in range(30):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone GADGET\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('iPhone is going on sale in London.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
